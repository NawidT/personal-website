{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "! source virtualenv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model_name='gpt-3.5-turbo', \n",
    "    temperature=0, \n",
    "    max_tokens=100, \n",
    "    openai_api_key=os.getenv('OPENAI_API_KEY')\n",
    ")\n",
    "\n",
    "text = \"Fast, good on the ball. Plays for Arsenal\"\n",
    "player_template = \"\"\"\n",
    "Pretend to be an energetic sports analyst. Return me a soccer player who is {text}.\n",
    "\"\"\"\n",
    "prompt_temp = PromptTemplate(input_variables=[\"text\"], template=player_template)\n",
    "chain = LLMChain(llm=llm, prompt=prompt_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.run(\"German midfielder\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ntahmid/Documents/Coding/Work/personal-website/virtualenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2315\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from llama_index import download_loader\n",
    "\n",
    "PyMuPDFReader = download_loader(\"PyMuPDFReader\")\n",
    "loader = PyMuPDFReader()\n",
    "documents_fast = loader.load(file_path=Path('../../../../Downloads/FormattedResume (1).pdf'), metadata=False)\n",
    "print(len(documents_fast[0].text))\n",
    "print(len(documents_fast))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "page_content='deep learning to predict traffic and economic damage from disasters. Combined Kaggle\\ndatasets, trained AI model, and visualized data with ReactJS and D3.js.\\nIntentional Design Studios\\nSoftware Engineering Intern May 2022 – August 2022 Atlanta, GA\\nOptimized Firestore database retrieval times by 35% to SvelteJS frontend by scripting accurate\\ndata requirements in TypeScript.Enhanced latency on loading and landing pages by engineering\\nlocal JavaScript and CSS interval-based animations\\nIntelligent Platforms for Crowdsourcing VIP\\nUndergraduate Researcher January 2021 – May 2023 Atlanta, GA\\nImplemented Naive Bayes classification to identify and encourage valuable comments on our\\ndebate hosting app. Manufactured a TF:IDF hashtag generator using NLP SpaCy with Python' metadata={}\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from llama_index.readers import Document\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=20)\n",
    "\n",
    "# convert llama_index Document to langchain Document\n",
    "texts = []\n",
    "for doc in documents_fast:\n",
    "    wrapper = Document(text=doc.text)\n",
    "    formatted = wrapper.to_langchain_format()\n",
    "    this_text = text_splitter.split_documents([formatted])\n",
    "    for text in this_text:\n",
    "        texts.append(text)\n",
    "\n",
    "# split documents to nodes/chunks\n",
    "print(len(texts))\n",
    "print(texts[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "manualdocs = [\n",
    "    Document(page_content=\"Name: Akhter (Nawid) Tahmid Number: +14042594142 Email: atahmid3@gatech.edu, Instagram: @nawid.tahmid  LinkedIn: https://www.linkedin.com/in/akhter-tahmid/, GitHub: https://github.com/nawidt\", metadata={\"page_number\": 1}),\n",
    "    Document(page_content=\"Job: Tesla, May 2024 to Aug 2024, Software Engineering Intern in Fremont, CA, Developed API utilized by 5,000 GSMs to score suppliers regarding greenhouse gas emissions\", metadata={\"page_number\": 2}),\n",
    "    Document(page_content=\"Job: Tektronix, Jan 2024 to May 2024, Artificial Intelligence Intern in Cleveland, OH. Tasks: Reduced 100,000 people hours searching for files with a Langchain RAG with sales/marketing data, Deployed a Flask Python API inside a Docker container on Azure App Service with CI/CD on Github Actions, Curated PowerBI dashboards utilizing DAX queries to monitor and test software adoption\", metadata={\"page_number\": 3}),\n",
    "    Document(page_content=\"Georgia Institute of Technology, August 2021 to December 2024, B.S. in Computer Science, Atlanta, GA GPA: 3.20/4.00 Major GPA: 3.53/4.00, Concentrations (Threads): Intelligence, Information Networks, Relevant Coursework: Algorithms Honors, Data Structures, Artificial Intelligence, Objects & Design, Systems & Networks, Applied Combinatorics, Databases, Statistics, Computer Organization, Linear Algebra, Discrete Mathematics\", metadata={\"page_number\": 4}),\n",
    "    Document(page_content=\"Job: Datasoft, ML Intern May 2023 to Aug 2023 in Dhaka, Bangladesh. Analyzed data, trained neural network models using PyTorch, Pandas, SQL, and Scikit-learn for IoT devices in fish farms. Improved livelihoods of thousands of farmers in Bangladesh. Predicted dissolved oxygen and ammonia levels, optimizing feeding rates and reducing costs by 30%. Job: ARQ Dreambox AI, Software Engineering Intern May 2023 to Present. (Remote) Atlanta, GA. Built the front-end of an innovative AI art-generated jewelry production company, utilizing React, Framer Motion, Hugging Face, and Flask to create an engaging and visually captivating user interface for showcasing personalized and stunning jewelry pieces. Job: Big Data Club. Project Lead August 2022 – May 2023 Atlanta, GA. Led team of 9 students to build web-app for plotting hurricanes and assessing economic impact. Organized SCRUM meetups, taught ReactJS and Pandas to improve skills. Used PyTorch for deep learning to predict traffic and economic damage from disasters. Combined Kaggle datasets, trained AI model, and visualized data with ReactJS and D3.js.\", metadata={\"page_number\": 5}),\n",
    "    Document(page_content=\"Job: Intentional Design Studios, Software Engineering Intern May 2022 to August 2022 Atlanta, GA. Optimized Firestore database retrieval times by 35% to SvelteJS frontend by scripting accurate data requirements in TypeScript.Enhanced latency on loading and landing pages by engineering local JavaScript and CSS interval-based animations. Job: Intelligent Platforms for Crowdsourcing VIP. Undergraduate Researcher January 2021 – May 2023 Atlanta, GA. Implemented Naive Bayes classification to identify and encourage valuable comments on our debate hosting app. Manufactured a TF:IDF hashtag generator using NLP SpaCy with Python by expunging stop-words and scoring terms.Generated a Flask REST API backend to connect hashtag model and valuable comment classifier to React.js front end using Python\", metadata={\"page_number\": 6}),\n",
    "    Document(page_content=\"Project: DayMaker using ReactJS, Express.js, Node.js, MongoDB, NLP, Google Cloud. Full-stack web application that automatically creates Google Calendar events from uploaded documents, using a custom-trained Named Entity Recognition model to extract dates and relevant titles. The project was awarded Winner of Best Use of Google Cloud among 400 teams at HackTX 2021. Project: Peacekeepers using Unreal Engine, C++, Blender. Led team of 5 to develop multiplayer FPS game. Managed progress with reports and GAANT charts. Implemented realistic 3D projectile motion. Optimized frame rate by 175% with culling, lighting, and LOD techniques. Automated geometry standardization to enhance productivity.\", metadata={\"page_number\": 7}),\n",
    "    Document(page_content=\"Languages: Python, Java, JavaScript, TypeScript, C++, Solidity, HTML/CSS, Frameworks: ReactJS, React Native, Node.js, Svelte.js, Express.js, Next.js, Flask, Tailwind, Chakra UI, SpaCy, Django, Langchain, Pinecone, HuggingFace Hub, Framer, Technologies: SQL, MongoDB, Postman, Firebase, Firestore, Docker, MATLAB, LaTeX, NumPy, Scikit-learn, Matplotlib, Pandas, Git, Unreal Engine, Blender\", metadata={\"page_number\": 8}),\n",
    "    Document(page_content=\"Love soccer, Arsenal fan since 2011. Watch MLS at Mercedes Benz Stadium. I love traveling in Atlanta, GA in Piedmont Park and Historic Fourth Ward Splash Park. I love keeping up with the newest tech news, especially in the LLM space. I’m invested in AI and ML. I have strong frontend and backend development skills. Organized person who prefers things neat and tidy. Born in Dhaka. Grew up in Qatar\", metadata={\"page_number\": 9}),\n",
    "    Document(page_content=\"Strengths: Neat and tidy, organized, love planning and scheduling tasks. I’m a fast learner. Organization leads to faster speed in the long term. Consistent performer who stays in the top echelon for longer periods of time. Weakness: I focus heavily on organizing which leads to a lot of time spent on planning and scheduling\", metadata={\"page_number\": 10})\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "import os\n",
    "from langchain.vectorstores.pinecone import Pinecone\n",
    "import pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Pinecone DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "\n",
    "try:\n",
    "    hostname = 'my-site-index-d691e93.svc.aped-4627-b74a.pinecone.io'\n",
    "    resolved_ip = socket.gethostbyname(hostname)\n",
    "    print(f\"Resolved IP for {hostname}: {resolved_ip}\")\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        openai_api_key=os.getenv('OPENAI_API_KEY'), \n",
    "        model='text-embedding-3-large',\n",
    "    )\n",
    "    pinecone.init(\n",
    "        api_key=\"810d90e9-333e-41f5-8dd5-da80cdeac681\",\n",
    "        host=\"https://my-site-index-d691e93.svc.aped-4627-b74a.pinecone.io\",\n",
    "        environment=\"us-east-1\",\n",
    "        project_name=\"Personal Resume\"\n",
    "    )\n",
    "\n",
    "    index = pinecone.Index(\"quickstart\")\n",
    "\n",
    "    vec_str = Pinecone(\n",
    "        index=index,\n",
    "        embedding_function=embeddings.embed_query,\n",
    "        text_key='page_content'\n",
    "    )\n",
    "\n",
    "    vec_str.add_documents(manualdocs)\n",
    "except Exception as e:\n",
    "    print(f\"Error resolving hostname {hostname}: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vdb = PineconeVectorStore.add_documents(manualdocs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine docs + query in Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain, ConversationChain, RetrievalQA\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=os.environ['OPENAI_API_KEY'])\n",
    "\n",
    "\n",
    "# creating the prompt for second chain\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Pretend you're Akhter (Nawid) Tahmid. Speak professionally. No complicated words. Answer in few short sentences: {question}?\"\n",
    ")\n",
    "\n",
    "llm = OpenAI(temperature=1, max_tokens=500, model=\"gpt-3.5\")\n",
    "mem = ConversationBufferWindowMemory(human_prefix=\"Nawid Tahmid\", k=3)\n",
    "mem.chat_memory.add_ai_message(\"Pretend you're Akhter (Nawid) Tahmid. Speak professionally. No complicated words. Answer in few short sentences\")\n",
    "\n",
    "\n",
    "# creating first chain for retrieval\n",
    "retr_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vec_str.as_retriever()\n",
    ")\n",
    "\n",
    "# creating second chain for answering\n",
    "ans_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=mem,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# creating the conversation chain\n",
    "comb_chain = SimpleSequentialChain(\n",
    "    chains=[retr_chain],\n",
    "    memory=mem,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Job: Datasoft, ML Intern May 2023 to Present in Dhaka, Bangladesh. Analyzed data, trained neural network models using PyTorch, Pandas, SQL, and Scikit-learn for IoT devices in fish farms. Improved livelihoods of thousands of farmers in Bangladesh. Predicted dissolved oxygen and ammonia levels, optimizing feeding rates and reducing costs by 30%. Job: ARQ Dreambox AI, Software Engineering Intern May 2023 to Present. (Remote) Atlanta, GA. Built the front-end of an innovative AI art-generated jewelry production company, utilizing React, Framer Motion, Hugging Face, and Flask to create an engaging and visually captivating user interface for showcasing personalized and stunning jewelry pieces. Job: Big Data Club. Project Lead August 2022 – May 2023 Atlanta, GA. Led team of 9 students to build web-app for plotting hurricanes and assessing economic impact. Organized SCRUM meetups, taught ReactJS and Pandas to improve skills. Used PyTorch for deep learning to predict traffic and economic damage from disasters. Combined Kaggle datasets, trained AI model, and visualized data with ReactJS and D3.js.', metadata={'page_number': 3.0})]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_str.similarity_search(\"tell me about your time at datasoft\", k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retr_feed = prompt.format(question=\"Who are you?\")\n",
    "retr_chain.run(retr_feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessageHistory(messages=[AIMessage(content=\"Pretend you're Akhter (Nawid) Tahmid. Speak professionally. No complicated words. Answer in few short sentences\", additional_kwargs={}, example=False)])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_chain.memory.chat_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m My name is Akhter (Nawid) Tahmid.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' My name is Akhter (Nawid) Tahmid.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_chain.run(\"whats ur name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import AIMessage\n",
    "\n",
    "memory = ConversationBufferWindowMemory()\n",
    "\n",
    "# Create an instance of the AIMessage\n",
    "ai_message = AIMessage(content=\"Your AI message here\")\n",
    "\n",
    "# Add the AI message to the memory\n",
    "memory.save_context({\"input\": \"Pretend to be Akhter (Nawid) Tahmid\"}, {\"output\": \"Hi, I'm Akhter (Nawid) Tahmid. A third year student at Georgia Tech. I speak professionally and don't use complex words.\"})\n",
    "\n",
    "doc_chain = StuffDocumentsChain(\n",
    "    llm_chain=LLMChain(\n",
    "        llm=ChatOpenAI(\n",
    "            temperature=0,\n",
    "            openai_api_key=os.getenv('OPENAI_API_KEY'),\n",
    "            model='gpt-3.5-turbo'\n",
    "        ),\n",
    "        memory=memory,\n",
    "        prompt=PromptTemplate.from_template(prompt_template)\n",
    "        \n",
    "    ) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am Akhter Tahmid, a Computer Science student at Georgia Institute of Technology from August 2021 to December 2024. My GPA is 3.20/4.00 and my major GPA is 3.53/4.00. My concentrations are Intelligence and Information Networks, and I have taken relevant coursework in Algorithms Honors, Data Structures, Artificial Intelligence, and more. I am a soccer fan and love traveling in Atlanta, GA. I am invested in AI and ML and have strong frontend and backend development skills. I am an organized person who prefers things neat and tidy. I was born in Dhaka and grew up in Qatar.\n"
     ]
    }
   ],
   "source": [
    "query = \"What school do you attend\"\n",
    "docs = docsem.similarity_search(query, k=2)\n",
    "print(doc_chain.run(input_documents=docs ,query=query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I think DayMaker is an impressive project. It is a full-stack web application that uses custom-trained Named Entity Recognition to automatically create Google Calendar events from uploaded documents. It was awarded Winner of Best Use of Google Cloud among 400 teams at HackTX 2021, which is a great accomplishment.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What do you think of DayMaker?\"\n",
    "docs = docsem.similarity_search(query) \n",
    "qa_chain.run(input_documents=docs, question=prompt_template.format(question=query))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
